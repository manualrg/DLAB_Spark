{
    "nbformat_minor": 1, 
    "cells": [
        {
            "source": "# ENRON EMAILS. TOPIC MODELLING", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": 16, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "import org.apache.spark.sql._\nimport org.apache.spark.sql.types._\nimport org.apache.spark.sql.functions._\n\nimport org.apache.spark.mllib.util.MLUtils\nimport org.apache.spark.ml.linalg.{Vector =>mlVector, Matrix=> mlMatrix}\nimport org.apache.spark.mllib.linalg.{Vector =>mllibVector, Matrix => mllibMatrix}\nimport org.apache.spark.mllib.linalg.distributed.RowMatrix\nimport breeze.linalg.{DenseMatrix=>BDM, DenseVector=>BDV}\nimport breeze.stats.mean \nimport breeze.linalg.{norm, normalize} \nimport breeze.linalg.functions.{cosineDistance, euclideanDistance}\n\n\nimport org.apache.spark.ml.{Pipeline, PipelineModel}\nimport org.apache.spark.ml.feature.{RegexTokenizer, StopWordsRemover,CountVectorizer, CountVectorizerModel,IDF, IDFModel}\nimport org.apache.spark.ml.clustering.{LDA, LDAModel}"
        }, 
        {
            "execution_count": 2, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "name": "stdout", 
                    "text": "Using cached version of spark-kernel-brunel-all-2.2.jar\n"
                }
            ], 
            "source": "//I recommend using Brunel 2.2 with Spark 2.1.\n%AddJar -magic https://brunelvis.org/jar/spark-kernel-brunel-all-2.2.jar"
        }, 
        {
            "source": "## 1. Load and Clean Data", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": 2, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "val docCleaner :String => String = doc => { doc.replaceAll(\"[^a-zA-Z0-9]\", \" \").replaceAll(\"\\\\s{2,}\", \" \").trim().toLowerCase() }\nval UDF_docCleaner = udf(docCleaner)"
        }, 
        {
            "execution_count": 3, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "display_data", 
                    "data": {
                        "text/plain": "Waiting for a Spark session to start..."
                    }, 
                    "metadata": {}
                }, 
                {
                    "output_type": "stream", 
                    "name": "stdout", 
                    "text": "First five documents: \n+---+--------------------+-----+--------------------+\n| id|               email|label|                 doc|\n+---+--------------------+-----+--------------------+\n|  1|North America's i...|  0.0|north america s i...|\n|  2|FYI -----Original...|  1.0|fyi original mess...|\n|  3|14:13:53 Synchron...|  0.0|14 13 53 synchron...|\n|  4|^ ----- Forwarded...|  1.0|forwarded by stev...|\n|  5|----- Forwarded b...|  0.0|forwarded by stev...|\n+---+--------------------+-----+--------------------+\n\n"
                }
            ], 
            "source": "val rdd = sc.textFile(\"enron_textfile.txt\").map(_.split(\"\\\\|\")).map(arr => (arr(0).toInt, arr(1), arr(2).toDouble) )\nval rowRDD = rdd.map(record => Row(record._1, record._2, record._3))\n\nval schema = new StructType().\n    add(StructField(\"id\", IntegerType, true)).\n    add(StructField(\"email\", StringType, true)).\n    add(StructField(\"label\", DoubleType, true))\n\nval corpus = spark.createDataFrame(rowRDD,schema).withColumn(\"doc\", UDF_docCleaner($\"email\"))\ncorpus.persist()\nprintln(\"First five documents: \")\ncorpus.limit(5).show()"
        }, 
        {
            "source": "## 2. Feature Enginnering", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "Document featurization is a process that maps every document in a vector space. The most basic technique is called BoW (Bag of Words) that builds the vector space by counting eact term-j frequency in document-i, so a document-term matrix is obtained. The number of terms taking into consideration is called vocabulary and it is usually set by a minimun token frequency.\n\nThis matrix has D rows (# documents) and V columns (# terms), where V is vocabulary size (V <= n, the total number of terms in the corpus). In Spark it will be represented by a DataFrame where the column features holds a collection of D vectors of size V.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": 4, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "name": "stdout", 
                    "text": "[Stage 2:=============================>                             (1 + 1) / 2]"
                }
            ], 
            "source": "val regexTokenizer = new RegexTokenizer().\n    setInputCol(\"doc\").\n    setOutputCol(\"tokens\").\n    setPattern(\"\\\\s+\").\n    setMinTokenLength(2)\nval remover = new StopWordsRemover().setCaseSensitive(false).setInputCol(\"tokens\").setOutputCol(\"tokens_rm\")\nval TF = new CountVectorizer().setInputCol(\"tokens_rm\").setOutputCol(\"features\").\n    setMinTF(2).\n    setVocabSize(500)\n\nval stages = Array(regexTokenizer, remover, TF)\nval feat_eng_pl = new Pipeline().setStages(stages).fit(corpus)\nval docTerm_df = feat_eng_pl.transform(corpus)\n"
        }, 
        {
            "execution_count": 5, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "name": "stdout", 
                    "text": "Vocabulary size: 500\nVocabulary: enron, ect, com, hou, power, 2000, subject, 2001, energy, mail\n"
                }
            ], 
            "source": "val TFModel = feat_eng_pl.stages(2).asInstanceOf[CountVectorizerModel]\nval vocabulary = TFModel.vocabulary\n\nprintln(\"Vocabulary size: \"+ vocabulary.size)\nprintln(\"Vocabulary: \"+ vocabulary.slice(0,10).mkString(\", \"))"
        }, 
        {
            "source": "## 3. Model Buidling", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "Topic modelling is an unsupervised technique that sets an a priori guess of how many different topics are there in a corpus (let's say K is the number of topics). A very basic explanation: Each topic will be related to a set of terms, (which are more frequent in that particular topic), any document will have words related to any of the k topics, but the frequency, number and importance of that set of terms given a topic will define the relationship of that term with the k topics.\n\nConsider the following example: Given a corpus of news, let's say that there is Sports, Science and Politics news, and by an initial guess we correctly set k=3.\n\nWe could expect the algorithm to find three topics, and a set of terms related to those three topics, those terms are ranked by importance (given by the weigths)\n\ntopic1: Obama(0.67), Congress(0.23), Democrats (0.08)\ntopic2: Messi(0.5), CR7(0.47), Referee(0.02)\ntopic3: Kepler(0.63), Sagan(0.27), Cassini(0.9)\n\nThe weights don't add up to 1 because there are other terms related to any topic, but in order to label or describe a topic, we only choose the most important ones.\n\nTherefore, any document is mapped to a vectorial space of k dimensions, as follows:\n\ndoc-i = [topic1=0.88, topic2=0.1, topic3=0.02]\n\nSo the most important topic in doc-i is topic1. Moreover, the algorithm also yields a rank of topics by topic importance in the whole corpus, let's say that each topic weight (or importance) in our corpus is k-p, p=1,2,...,K:\n\n[k1,k3, k2] means that topic k1 is more important that k3 and this one is more important than k2.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "Fitting a LDA (latent Dirichlet allocation):\n\nUse SVD (Singular Value Decompostion) in order to obtain an aproximation of the docTerm Matrix [M] so that:\n\n$$ M = U x \\Sigma x V^{T} $$\n\n[U]: each row is a document i=1,...,D , and every column is a topic, j=1,...,k (dxk). It maps every document in a topics vectorial space. Used to cluster documents by topic\n\n[V]: each row is a term j=1,...,n and every column is a topic i=1,...k  (nxk). It maps every topic features vector space. It's main task is to label (or describe) each topic with the most relevant terms\n\n[Sigma]: Diagonal matrix of topic coefficients (kxk). Rank the most important topics", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": 6, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "name": "stdout", 
                    "text": "[Stage 8:=============================>                             (1 + 1) / 2]+-----+------------+------------------------------------------------------------------+\n|topic|termIndices |termWeights                                                       |\n+-----+------------+------------------------------------------------------------------+\n|0    |[2, 9, 0]   |[0.21741384290888094, 0.049706981136568226, 0.04053549921057427]  |\n|1    |[10, 14, 15]|[0.025309305907768573, 0.019168714028749912, 0.018024278035226916]|\n|2    |[0, 1, 3]   |[0.09614295286779202, 0.09455567268586144, 0.04721695071499122]   |\n+-----+------------+------------------------------------------------------------------+\n\n"
                }
            ], 
            "source": "val lda = new LDA().setK(3).setMaxIter(30).setFeaturesCol(\"features\").setTopicDistributionCol(\"topicDistribution\")\nval ldaModel = lda.fit(docTerm_df) //org.apache.spark.ml.clustering.LDA\nval topicsTop_df = ldaModel.describeTopics(3)\ntopicsTop_df.show(false)"
        }, 
        {
            "execution_count": 7, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "name": "stdout", 
                    "text": "+---+--------------------+--------------------+\n| id|                 doc|   topicDistribution|\n+---+--------------------+--------------------+\n|  1|north america s i...|[0.00235965459353...|\n|  2|fyi original mess...|[0.82798160872248...|\n|  3|14 13 53 synchron...|[0.05753786926167...|\n|  4|forwarded by stev...|[0.59561274836540...|\n|  5|forwarded by stev...|[0.24381207134737...|\n+---+--------------------+--------------------+\nonly showing top 5 rows\n\n"
                }
            ], 
            "source": "val docTopic_df = ldaModel.transform(docTerm_df)\ndocTopic_df.select($\"id\", $\"doc\", $\"topicDistribution\").show(5)"
        }, 
        {
            "source": "The deafult Optimizer yields a LDAModel that is local, it stores information about topics only, not about the training dataset", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": 8, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "execution_count": 8, 
                    "metadata": {}, 
                    "data": {
                        "text/plain": "false"
                    }, 
                    "output_type": "execute_result"
                }
            ], 
            "source": "ldaModel.isDistributed"
        }, 
        {
            "source": "### Map topics in features space and compute distances among them", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": 9, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "val n = ldaModel.vocabSize"
        }, 
        {
            "execution_count": 10, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "name": "stdout", 
                    "text": "Terms coordinates in topic vectorial space:\n"
                }
            ], 
            "source": "//Inferred topics, where each topic is represented by a distribution over terms. (Local Matrix: nxk, each column is a topic)\nprintln(\"Terms coordinates in topic vectorial space:\")\nval topicMat = ldaModel.topicsMatrix\nval topicExpArr = topicMat.toArray"
        }, 
        {
            "execution_count": 11, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "val xTopic0 = BDV(topicExpArr.slice(0,n))\nval xTopic1 = BDV(topicExpArr.slice(n,2*n))\nval xTopic2 = BDV(topicExpArr.slice(2*n,topicExpArr.size+1))\n"
        }, 
        {
            "execution_count": 12, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "val zTopic0 =  normalize(xTopic0) \nval zTopic1 =  normalize(xTopic1)\nval zTopic2 =  normalize(xTopic2)"
        }, 
        {
            "execution_count": 18, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "name": "stdout", 
                    "text": "Distance in features vectorial space from topic0 to topic1 coordinates: \nEuclidean: 0.8395039621796017\nCosine similarity: 1.29576538167957\n"
                }
            ], 
            "source": "val norm_eucldist_01 = cosineDistance(zTopic0,zTopic1)\nval norm_cosdist_01 = euclideanDistance(zTopic0, zTopic1) \nprintln(\"Distance in features vectorial space from topic0 to topic1 coordinates: \")\nprintln(\"Euclidean: \"+ norm_eucldist_01)\nprintln(\"Cosine similarity: \"+ norm_cosdist_01)"
        }, 
        {
            "execution_count": 21, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "name": "stdout", 
                    "text": "Distance in features vectorial space from topic0 to topic2 coordinates: \nEuclidean: 0.8398693833223239\nCosine similarity: 1.2960473628091869\n"
                }
            ], 
            "source": "val norm_eucldist_02 = cosineDistance(zTopic0, zTopic2) \nval norm_cosdist_02 = euclideanDistance(zTopic0, zTopic2)\nprintln(\"Distance in features vectorial space from topic0 to topic2 coordinates: \")\nprintln(\"Euclidean: \"+ norm_eucldist_02)\nprintln(\"Cosine similarity: \"+ norm_cosdist_02)"
        }, 
        {
            "execution_count": 22, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "name": "stdout", 
                    "text": "Distance in features vectorial space from topic0 to topic1 coordinates: \nEuclidean: 0.6462130127326077\nCosine similarity: 1.1368491656614865\n"
                }
            ], 
            "source": "val norm_eucldist_12 = cosineDistance(zTopic1, zTopic2)\nval norm_cosdist_12 = euclideanDistance(zTopic1, zTopic2)\nprintln(\"Distance in features vectorial space from topic0 to topic1 coordinates: \")\nprintln(\"Euclidean: \"+ norm_eucldist_12)\nprintln(\"Cosine similarity: \"+ norm_cosdist_12)"
        }, 
        {
            "source": "### Label topics with the most relevant words", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": 52, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "name": "stdout", 
                    "text": "+-----+---+------+\n|topic|pos|token |\n+-----+---+------+\n|0    |0  |com   |\n|0    |1  |mail  |\n|0    |2  |enron |\n|1    |0  |power |\n|1    |1  |state |\n|1    |2  |market|\n|2    |0  |enron |\n|2    |1  |ect   |\n|2    |2  |hou   |\n+-----+---+------+\n\n"
                }
            ], 
            "source": "val topicTokens = topicsTop_df.select($\"topic\", posexplode($\"termIndices\"), $\"termWeights\").withColumnRenamed(\"col\",\"tokenIdx\").\n    withColumn(\"vocabulary\",lit(vocabulary)).select($\"topic\",$\"pos\",expr(\"vocabulary[tokenIdx] as token\"))\n\n topicTokens.show(false)"
        }, 
        {
            "execution_count": 53, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "name": "stdout", 
                    "text": "+-----+---+--------------------+\n|topic|pos|tokenIdx            |\n+-----+---+--------------------+\n|0    |0  |0.21436202279104347 |\n|0    |1  |0.04889714586407715 |\n|0    |2  |0.03035895022908159 |\n|1    |0  |0.03175053985006113 |\n|1    |1  |0.019002641638193714|\n|1    |2  |0.0181822689958963  |\n|2    |0  |0.11093990448578811 |\n|2    |1  |0.10631141311387063 |\n|2    |2  |0.05316370797273026 |\n+-----+---+--------------------+\n\n"
                }
            ], 
            "source": "val topicCoefs = topicsTop_df.select($\"topic\",posexplode($\"termWeights\")).withColumnRenamed(\"col\",\"tokenIdx\")\ntopicCoefs.show(false)"
        }, 
        {
            "execution_count": 54, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "name": "stdout", 
                    "text": "Topic labelling: \n+-----+---+------+--------------------+\n|topic|pos|token |tokenIdx            |\n+-----+---+------+--------------------+\n|0    |0  |com   |0.21436202279104347 |\n|0    |1  |mail  |0.04889714586407715 |\n|0    |2  |enron |0.03035895022908159 |\n|1    |0  |power |0.03175053985006113 |\n|1    |1  |state |0.019002641638193714|\n|1    |2  |market|0.0181822689958963  |\n|2    |0  |enron |0.11093990448578811 |\n|2    |1  |ect   |0.10631141311387063 |\n|2    |2  |hou   |0.05316370797273026 |\n+-----+---+------+--------------------+\n\n"
                }
            ], 
            "source": "val topicReport =  topicTokens.join(topicCoefs,Seq(\"topic\",\"pos\"),\"inner\")\nprintln(\"Topic labelling: \")\ntopicReport.show(false)"
        }, 
        {
            "source": "We can see that topic0 is closely related to [com, email, enron], topic1 to [power, market, state] and topic2=[enron, ect, hou]\n\nBy checking at what tokens best describe a topic, we can describe (or label) that topic. Topic 1 is closely related to energy, so it will be interesting in trial, however topics 0 and 2 may be not so interesting.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "### Cluster and rank documents most closely related to every topic", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "To leverage topic modelling we need to perform two main tasks:\n* Clustering documenints in topics\n* In each cluster, rank documents by it's relative importance", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": 55, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "val maxElementIdx = (v: Vector) => v.toArray.zipWithIndex.maxBy(_._1)._2 :Int \nval maxElementIdx_UDF = udf(maxElementIdx)"
        }, 
        {
            "execution_count": 56, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "val maxElement = (v: Vector) => v.toArray.max.toDouble :Double\nval maxElement_UDF = udf(maxElement)"
        }, 
        {
            "execution_count": 138, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "name": "stdout", 
                    "text": "Documents coordinates in topics vectorial space: \n+---+--------------------------------------------------------------+\n|id |topicDistribution                                             |\n+---+--------------------------------------------------------------+\n|1  |[0.04876631962014416,0.9474722648407216,0.003761415539134295] |\n|2  |[0.6728750600791644,0.005257764192483943,0.3218671757283516]  |\n|3  |[0.9880542759065452,4.522598245078323E-4,0.011493464268947028]|\n|4  |[0.4709270676557802,0.011854241218315012,0.5172186911259047]  |\n|5  |[0.22301099524302745,0.21453950505845448,0.562449499698518]   |\n+---+--------------------------------------------------------------+\n\n"
                }
            ], 
            "source": "val docTopic = docTopic_df.withColumn(\"relatedTopic\", maxElementIdx_UDF($\"topicDistribution\")).\n    withColumn(\"relatedTopicWeight\", maxElement_UDF($\"topicDistribution\"))\n//Every topic importance among documents\nprintln(\"Documents coordinates in topics vectorial space: \")\ndocTopic.select($\"id\",$\"topicDistribution\").limit(5).show(false)"
        }, 
        {
            "execution_count": 58, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "name": "stdout", 
                    "text": "[Stage 303:============================>                            (1 + 1) / 2]+---+------------+------------------+\n|id |relatedTopic|relatedTopicWeight|\n+---+------------+------------------+\n|483|0           |0.9992277264915075|\n|465|0           |0.9991443968816465|\n|337|0           |0.9985810353565892|\n|530|0           |0.9983450275653591|\n|412|0           |0.9978456459393924|\n|592|0           |0.9973319241233644|\n|9  |0           |0.9972315506201558|\n|542|0           |0.9969824379044203|\n|581|0           |0.9958069633704406|\n|641|0           |0.9950648492781391|\n+---+------------+------------------+\n\n+---+------------+------------------+                                           \n|id |relatedTopic|relatedTopicWeight|\n+---+------------+------------------+\n|527|1           |0.9990804429656244|\n|707|1           |0.9989610219249793|\n|531|1           |0.9980240876424809|\n|838|1           |0.9973335321156257|\n|283|1           |0.996504936858759 |\n|806|1           |0.9963827172460834|\n|848|1           |0.9961773152338987|\n|479|1           |0.994932511579049 |\n|218|1           |0.9949145981555191|\n|418|1           |0.9945815476576008|\n+---+------------+------------------+\n\n+---+------------+------------------+\n|id |relatedTopic|relatedTopicWeight|\n+---+------------+------------------+\n|611|2           |0.9993432680953913|\n|235|2           |0.9991385407628168|\n|652|2           |0.9986699345984491|\n|528|2           |0.9983846638029392|\n|663|2           |0.9981449476800898|\n|767|2           |0.9977147769343486|\n|354|2           |0.997463307907778 |\n|558|2           |0.9973796387942786|\n|764|2           |0.9973139649034769|\n|791|2           |0.997283101683552 |\n+---+------------+------------------+\n\n"
                }, 
                {
                    "execution_count": 58, 
                    "metadata": {}, 
                    "data": {
                        "text/plain": "Vector((), (), ())"
                    }, 
                    "output_type": "execute_result"
                }
            ], 
            "source": "//Most closely related topic to every document\nRange(0,3).map(x => docTopic.filter($\"relatedTopic\" === lit(x)).\n                                       select($\"id\",  $\"relatedTopic\", $\"relatedTopicWeight\").\n                                       orderBy($\"relatedTopicWeight\".desc).\n                                       limit(10).show(false))\n"
        }, 
        {
            "source": "### Analyze response variable in each topic", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "Topic modelling is an unsupervised technique, however in this case, we have a response variable (label) and we can check if splitting the corpus in topics yields better event proportions in  each topic (or cluster of documents)", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": 59, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "name": "stdout", 
                    "text": "Classification analyisis by topic: \n[Stage 309:==========================================>              (3 + 1) / 4]+------------+---+----+-----+-------------------+------------------+\n|relatedTopic|  N|  N1|   N0|                  e|                ne|\n+------------+---+----+-----+-------------------+------------------+\n|           1|166|72.0| 94.0|0.43373493975903615|0.5662650602409639|\n|           2|389|47.0|342.0|0.12082262210796915|0.8791773778920309|\n|           0|300|20.0|280.0|0.06666666666666667|0.9333333333333333|\n+------------+---+----+-----+-------------------+------------------+\n\n"
                }
            ], 
            "source": "val docTopic_analytics = docTopic.groupBy($\"relatedTopic\").agg(count(\"*\").as(\"N\"), sum(\"label\").as(\"N1\")).\n    withColumn(\"N0\", $\"N\"-$\"N1\").\n    withColumn(\"e\", $\"N1\"/$\"N\").\n    withColumn(\"ne\", $\"N0\"/$\"N\")\nprintln(\"Classification analyisis by topic: \")\ndocTopic_analytics.show()"
        }, 
        {
            "source": "In fact, event proportion in topic 1 [powe, market, state] is the highest, so it is  an easy to understand way of clustering documents and therefore to classify them. Moreover, topics 1 and 2 yield a lower event proportion that the baseline (about 0.16), so LDA with k=3 is very good performant unsupervised model to achieve text classification.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": ""
        }
    ], 
    "metadata": {
        "kernelspec": {
            "display_name": "Scala 2.11 with Spark 2.1", 
            "name": "scala-spark21", 
            "language": "scala"
        }, 
        "language_info": {
            "mimetype": "text/x-scala", 
            "version": "2.11.8", 
            "name": "scala", 
            "pygments_lexer": "scala", 
            "file_extension": ".scala", 
            "codemirror_mode": "text/x-scala"
        }
    }, 
    "nbformat": 4
}