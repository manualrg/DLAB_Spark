{
    "cells": [
        {
            "cell_type": "markdown", 
            "source": "# ENRON EMAILS: ADVANCED FEATURE ENGINEERING AND MODEL BUILDING TECHNIQUES", 
            "metadata": {}
        }, 
        {
            "cell_type": "code", 
            "execution_count": 1, 
            "metadata": {}, 
            "outputs": [], 
            "source": "import org.apache.spark.sql._\nimport org.apache.spark.sql.functions._\nimport org.apache.spark.sql.types._\nimport org.apache.spark.ml.{Pipeline, PipelineModel}\nimport org.apache.spark.ml.feature.{VectorAssembler, ChiSqSelector, RegexTokenizer, StopWordsRemover, CountVectorizer, CountVectorizerModel, IDF, NGram, Word2Vec, Word2VecModel}\nimport org.apache.spark.ml.classification.{LogisticRegression, LogisticRegressionModel, RandomForestClassifier, RandomForestClassificationModel}\nimport org.apache.spark.ml.tuning.{CrossValidator, ParamGridBuilder }\nimport org.apache.spark.ml.param.ParamMap\nimport org.apache.spark.ml.evaluation.BinaryClassificationEvaluator\nimport org.apache.spark.mllib.evaluation.BinaryClassificationMetrics\n\nimport org.apache.spark.ml.linalg._"
        }, 
        {
            "cell_type": "code", 
            "execution_count": 2, 
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "text": "Using cached version of spark-kernel-brunel-all-2.2.jar\n", 
                    "name": "stdout"
                }
            ], 
            "source": "//I recommend using Brunel 2.2 with Spark 2.1.\n%AddJar -magic https://brunelvis.org/jar/spark-kernel-brunel-all-2.2.jar"
        }, 
        {
            "cell_type": "markdown", 
            "source": "## 1. Load in Data", 
            "metadata": {}
        }, 
        {
            "cell_type": "code", 
            "execution_count": 3, 
            "metadata": {}, 
            "outputs": [
                {
                    "execution_count": 3, 
                    "metadata": {}, 
                    "output_type": "execute_result", 
                    "data": {
                        "text/plain": "Array((1,North America's integrated electricity market requires cooperation on environmental policies Commission for Environmental Cooperation releases working paper on North America's electricity market Montreal, 27 November 2001 -- The North American Commission for Environmental Cooperation (CEC) is releasing a working paper highlighting the trend towards increasing trade, competition and cross-border investment in electricity between Canada, Mexico and the United States. It is hoped that the working paper, Environmental Challenges and Opportunities in the Evolving North American Electricity Market, will stimulate public discussion around a CEC symposium of the same title about the need to coordinate environmental policies trinationally as a North..."
                    }
                }
            ], 
            "source": "val rdd = sc.textFile(\"enron_textfile.txt\").map(_.split(\"\\\\|\")).map(arr => (arr(0).toInt, arr(1), arr(2).toDouble) )\nrdd.take(1)"
        }, 
        {
            "cell_type": "code", 
            "execution_count": 4, 
            "metadata": {}, 
            "outputs": [], 
            "source": "val docCleaner :String => String = doc => { doc.replaceAll(\"[^a-zA-Z0-9]\", \" \").replaceAll(\"\\\\s{2,}\", \" \").trim().toLowerCase() }\nval UDF_docCleaner = udf(docCleaner)"
        }, 
        {
            "cell_type": "code", 
            "execution_count": 5, 
            "metadata": {}, 
            "outputs": [], 
            "source": "val schema = new StructType().\n    add(StructField(\"id\", IntegerType, true)).\n    add(StructField(\"email\", StringType, true)).\n    add(StructField(\"label\", DoubleType, true))"
        }, 
        {
            "cell_type": "code", 
            "execution_count": 6, 
            "metadata": {}, 
            "outputs": [], 
            "source": "val rowRDD = rdd.map(record => Row(record._1, record._2, record._3))"
        }, 
        {
            "cell_type": "code", 
            "execution_count": 7, 
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "text": "First five documents: \n+---+--------------------+-----+--------------------+\n| id|               email|label|                 doc|\n+---+--------------------+-----+--------------------+\n|  1|North America's i...|  0.0|north america s i...|\n|  2|FYI -----Original...|  1.0|fyi original mess...|\n|  3|14:13:53 Synchron...|  0.0|14 13 53 synchron...|\n|  4|^ ----- Forwarded...|  1.0|forwarded by stev...|\n|  5|----- Forwarded b...|  0.0|forwarded by stev...|\n+---+--------------------+-----+--------------------+\n\n", 
                    "name": "stdout"
                }
            ], 
            "source": "val corpus = spark.createDataFrame(rowRDD,schema).withColumn(\"doc\", UDF_docCleaner($\"email\"))\ncorpus.persist()\nprintln(\"First five documents: \")\ncorpus.limit(5).show()"
        }, 
        {
            "cell_type": "code", 
            "execution_count": 8, 
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "text": "Label analysis:\n+-----+---+-----+-------------------+------------------+\n|   N1|  N|   N0|         event_rate| baseline_Accuracy|\n+-----+---+-----+-------------------+------------------+\n|139.0|855|716.0|0.16257309941520467|0.8374269005847953|\n+-----+---+-----+-------------------+------------------+\n\n", 
                    "name": "stdout"
                }
            ], 
            "source": "val stats = corpus.agg(sum(\"label\").as(\"N1\"), count(\"*\").as(\"N\")).\n    withColumn(\"N0\", $\"N\"-$\"N1\").\n    withColumn(\"event_rate\", $\"N1\"/$\"N\").withColumn(\"baseline_Accuracy\", when($\"N0\">$\"N1\", $\"N0\").otherwise($\"N1\")/$\"N\")\nprintln(\"Label analysis:\")\nstats.show()"
        }, 
        {
            "cell_type": "markdown", 
            "source": "## 2. Feature Engineering", 
            "metadata": {}
        }, 
        {
            "cell_type": "code", 
            "execution_count": 9, 
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "text": "[Stage 1914:============================>                           (1 + 1) / 2]", 
                    "name": "stdout"
                }
            ], 
            "source": "val regexTokenizer = new RegexTokenizer().\n    setInputCol(\"doc\").\n    setOutputCol(\"tokens\").\n    setPattern(\"\\\\s+\").\n    setMinTokenLength(2)\nval remover = new StopWordsRemover().setCaseSensitive(false).setInputCol(\"tokens\").setOutputCol(\"tokens_rm\")\n\nval TF_1gram = new CountVectorizer().setInputCol(\"tokens_rm\").setOutputCol(\"rawFeatures_1gram\").\n    setMinTF(2).\n    setVocabSize(500)\nval idf_1gram = new IDF().setInputCol(\"rawFeatures_1gram\").setOutputCol(\"TFIDF1gramFeatures\")\n\nval _2grams = new NGram().setInputCol(\"tokens_rm\").setOutputCol(\"tokens_2gram\").setN(2)\nval TF_2gram = new CountVectorizer().setInputCol(\"tokens_2gram\").setOutputCol(\"rawFeatures_2gram\").\n    setMinTF(2).\n    setVocabSize(500)\nval idf_2gram = new IDF().setInputCol(\"rawFeatures_2gram\").setOutputCol(\"TFIDF2gramFeatures\")\n\nval TF_2gramBinary = new CountVectorizer().setInputCol(\"tokens_2gram\").setOutputCol(\"rawFeatures_2gramBin\").\n    setMinTF(2).\n    setBinary(true).\n    setVocabSize(100)\nval chiSqSelector = new ChiSqSelector().setFeaturesCol(\"rawFeatures_2gramBin\").setLabelCol(\"label\").setOutputCol(\"TFBin2gramFeaturesChiSq\").setSelectorType(\"percentile\").setPercentile(0.1)\n\nval featAssembler = new VectorAssembler().setInputCols(Array(\"TFIDF1gramFeatures\", \"rawFeatures_2gramBin\")).setOutputCol(\"TFIDFcombiFeatures\")\n\nval basicFeatEng_stages = Array(regexTokenizer, remover, TF_1gram, idf_1gram, _2grams, TF_2gram, idf_2gram, TF_2gramBinary, chiSqSelector,featAssembler)\nval basicFeatEng_pl = new Pipeline().setStages(basicFeatEng_stages).fit(corpus)"
        }, 
        {
            "cell_type": "code", 
            "execution_count": 10, 
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "text": "root\n |-- id: integer (nullable = true)\n |-- email: string (nullable = true)\n |-- label: double (nullable = true)\n |-- doc: string (nullable = true)\n |-- tokens: array (nullable = true)\n |    |-- element: string (containsNull = true)\n |-- tokens_rm: array (nullable = true)\n |    |-- element: string (containsNull = true)\n |-- rawFeatures_1gram: vector (nullable = true)\n |-- TFIDF1gramFeatures: vector (nullable = true)\n |-- tokens_2gram: array (nullable = true)\n |    |-- element: string (containsNull = false)\n |-- rawFeatures_2gram: vector (nullable = true)\n |-- TFIDF2gramFeatures: vector (nullable = true)\n |-- rawFeatures_2gramBin: vector (nullable = true)\n |-- TFBin2gramFeaturesChiSq: vector (nullable = true)\n |-- TFIDFcombiFeatures: vector (nullable = true)\n\n", 
                    "name": "stdout"
                }
            ], 
            "source": "val basicFeatEng_docTerm =  basicFeatEng_pl.transform(corpus)\nbasicFeatEng_docTerm.persist()\nbasicFeatEng_docTerm.printSchema()"
        }, 
        {
            "cell_type": "code", 
            "execution_count": 11, 
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "text": "enron\nect\ncom\nhou\npower\n2000\nsubject\n2001\nenergy\nmail\n", 
                    "name": "stdout"
                }
            ], 
            "source": "val _1gramVocabulary = basicFeatEng_pl.stages(2).asInstanceOf[CountVectorizerModel].vocabulary\n_1gramVocabulary.slice(0,10).foreach(println)"
        }, 
        {
            "cell_type": "code", 
            "execution_count": 12, 
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "text": "ect ect\nhou ect\nenron enron\nenron com\ncc subject\ncorp enron\nna enron\nenron development\nect cc\nees ees\n", 
                    "name": "stdout"
                }
            ], 
            "source": "val _2gramVocabulary = basicFeatEng_pl.stages(5).asInstanceOf[CountVectorizerModel].vocabulary\n_2gramVocabulary.slice(0,10).foreach(println)"
        }, 
        {
            "cell_type": "code", 
            "execution_count": 13, 
            "metadata": {}, 
            "outputs": [], 
            "source": "val split_weights = Array(0.7,0.3)\nval splits = basicFeatEng_docTerm.randomSplit(split_weights, 123).zip(Array(\"train\",\"test\"))\nval train = splits(0)._1\nval test = splits(1)._1"
        }, 
        {
            "cell_type": "markdown", 
            "source": "## 3. Model Building and word2vec featurization", 
            "metadata": {}
        }, 
        {
            "cell_type": "markdown", 
            "source": "### Ngram Pipelines", 
            "metadata": {}
        }, 
        {
            "cell_type": "code", 
            "execution_count": 14, 
            "metadata": {}, 
            "outputs": [], 
            "source": "val cv_names = Array(\"1gram\", \"2gram\", \"combi\")"
        }, 
        {
            "cell_type": "code", 
            "execution_count": 15, 
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "text": "[Stage 1986:==========================================>             (3 + 1) / 4]\n1gram\n({\n\trfc_932fd31a16ad-maxDepth: 6\n},0.8687454634010725)\n({\n\trfc_932fd31a16ad-maxDepth: 9\n},0.8597984709533524)\n({\n\trfc_932fd31a16ad-maxDepth: 3\n},0.8505288738762268)\n\n2gram\n({\n\trfc_b69c3a5b4b1c-maxDepth: 9\n},0.7282307525606881)\n({\n\trfc_b69c3a5b4b1c-maxDepth: 6\n},0.7181896844765344)\n({\n\trfc_b69c3a5b4b1c-maxDepth: 3\n},0.697259190464961)\n\ncombi\n({\n\trfc_dc81a834333a-maxDepth: 9\n},0.867121910553916)\n({\n\trfc_dc81a834333a-maxDepth: 6\n},0.8653881242445118)\n({\n\trfc_dc81a834333a-maxDepth: 3\n},0.8514992759213362)\n", 
                    "name": "stdout"
                }
            ], 
            "source": "val cv_ngrams = cv_names.map(name => {val rf = new RandomForestClassifier().setLabelCol(\"label\").setFeaturesCol(\"TFIDF\" + name + \"Features\").setNumTrees(20)\n                                val paramGrid= new ParamGridBuilder().addGrid(rf.maxDepth, Array(3, 6, 9)).build()\n                                val binClassEval = new BinaryClassificationEvaluator().setLabelCol(\"label\").setRawPredictionCol(\"rawPrediction\").setMetricName(\"areaUnderROC\")\n                                val cv = new CrossValidator().setEstimator(rf).setEstimatorParamMaps(paramGrid).setEvaluator(binClassEval).setNumFolds(3).fit(train)\n                                val bestModel = cv.bestModel\n                                val cvReport = cv.getEstimatorParamMaps.zip(cv.avgMetrics).sortBy(-_._2)\n                                println(\"\")\n                                println(name)\n                                cvReport.foreach(println)\n                               (name, cv)\n                              })"
        }, 
        {
            "cell_type": "markdown", 
            "source": "### Word2Vec pipeline", 
            "metadata": {}
        }, 
        {
            "cell_type": "markdown", 
            "source": "Word2Vec computes distributed vector representation of words, taking into account semantics relationships between words, through context window analysis and a neural network model. Word2Vec maps every word to a vector that represent that word in a feature space, encapsulating semanting relationships\n\nThere are two main approaches:\n* CBOW: Continous Bag Of Words: Predict a word from its context\n* Skip-gram: Predict a context from each word\n\nThe neural network only has one hidden layer and the vector representation is gathered from neuron weights. If two words have similar context, (therefore similar meaning) they will be represented by similar vectors", 
            "metadata": {}
        }, 
        {
            "cell_type": "code", 
            "execution_count": 16, 
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "text": "[Stage 3431:============================>                           (1 + 1) / 2]", 
                    "name": "stdout"
                }
            ], 
            "source": "//estimator\nval word2Vec = new Word2Vec().\n    setInputCol(\"tokens_rm\").\n    setOutputCol(\"w2vFeatures\").\n    setMinCount(5).\n    setWindowSize(5)\n//estimator\nval rf_w2v = new RandomForestClassifier().setLabelCol(\"label\").setFeaturesCol(\"w2vFeatures\").setNumTrees(20)\n//grid param\nval paramGrid_w2v = new ParamGridBuilder().addGrid(rf_w2v.maxDepth, Array(3, 6, 9)).addGrid(word2Vec.vectorSize, Array(10, 20, 30)).build()\n//evaluator                                                   \nval binClassEval = new BinaryClassificationEvaluator().setLabelCol(\"label\").setRawPredictionCol(\"rawPrediction\").setMetricName(\"areaUnderROC\")\n                                                   \nval w2vStages = Array(word2Vec, rf_w2v)\nval w2v_pl = new Pipeline().setStages(w2vStages)\nval cv_w2v = new CrossValidator().setEstimator(w2v_pl).setEstimatorParamMaps(paramGrid_w2v).setEvaluator(binClassEval).setNumFolds(3).fit(basicFeatEng_docTerm)"
        }, 
        {
            "cell_type": "code", 
            "execution_count": 17, 
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "text": "({\n\trfc_dd40def74c5d-maxDepth: 3,\n\tw2v_6062a1fa540a-vectorSize: 20\n},0.8418809269838038)\n({\n\trfc_dd40def74c5d-maxDepth: 6,\n\tw2v_6062a1fa540a-vectorSize: 30\n},0.8412259838714485)\n({\n\trfc_dd40def74c5d-maxDepth: 6,\n\tw2v_6062a1fa540a-vectorSize: 20\n},0.8387943326279337)\n({\n\trfc_dd40def74c5d-maxDepth: 9,\n\tw2v_6062a1fa540a-vectorSize: 20\n},0.8350356015024923)\n({\n\trfc_dd40def74c5d-maxDepth: 3,\n\tw2v_6062a1fa540a-vectorSize: 30\n},0.8306138328289846)\n({\n\trfc_dd40def74c5d-maxDepth: 9,\n\tw2v_6062a1fa540a-vectorSize: 30\n},0.8274166202443218)\n({\n\trfc_dd40def74c5d-maxDepth: 3,\n\tw2v_6062a1fa540a-vectorSize: 10\n},0.8264827398020201)\n({\n\trfc_dd40def74c5d-maxDepth: 6,\n\tw2v_6062a1fa540a-vectorSize: 10\n},0.8152066378386517)\n({\n\trfc_dd40def74c5d-maxDepth: 9,\n\tw2v_6062a1fa540a-vectorSize: 10\n},0.8073612937327077)\n", 
                    "name": "stdout"
                }
            ], 
            "source": "val bestModel_w2v = cv_w2v.bestModel\nval cvReport_w2v = cv_w2v.getEstimatorParamMaps.zip(cv_w2v.avgMetrics).sortBy(-_._2)\nprintln(\"\")\ncvReport_w2v.foreach(println)"
        }, 
        {
            "cell_type": "markdown", 
            "source": "## 4. Model Assessment", 
            "metadata": {}
        }, 
        {
            "cell_type": "code", 
            "execution_count": 30, 
            "metadata": {}, 
            "outputs": [], 
            "source": "val cv_array :Array[(String, org.apache.spark.ml.tuning.CrossValidatorModel)] = cv_ngrams ++ Array( (\"w2v\", cv_w2v) )"
        }, 
        {
            "cell_type": "code", 
            "execution_count": 31, 
            "metadata": {}, 
            "outputs": [
                {
                    "execution_count": 31, 
                    "metadata": {}, 
                    "output_type": "execute_result", 
                    "data": {
                        "text/plain": "Array(evalRecord([id: int, email: string ... 15 more fields],cv_079ef8a4491d,combi,train,0.9766666666666665), evalRecord([id: int, email: string ... 15 more fields],cv_9fd85a2de99e,1gram,train,0.958122489959839), evalRecord([id: int, email: string ... 16 more fields],cv_2a772717c894,w2v,train,0.9057329317269073), evalRecord([id: int, email: string ... 16 more fields],cv_2a772717c894,w2v,test,0.8877911079745943), evalRecord([id: int, email: string ... 15 more fields],cv_cee8ee5a8b6d,2gram,train,0.8461144578313252), evalRecord([id: int, email: string ... 15 more fields],cv_9fd85a2de99e,1gram,test,0.8380969183721478), evalRecord([id: int, email: string ... 15 more fields],cv_079ef8a4491d,combi,test,0.7928134556574926), evalRecord([id: int, email: string ... 15 m..."
                    }
                }
            ], 
            "source": "case class evalRecord(predictions :org.apache.spark.sql.DataFrame, model :org.apache.spark.ml.tuning.CrossValidatorModel, modelDesc :String, dataset :String, evalMetric :Double)\n\nval binEval = new BinaryClassificationEvaluator().setRawPredictionCol(\"rawPrediction\").setLabelCol(\"label\").setMetricName(\"areaUnderROC\")\n\ndef CVEvaluatorBundle(binEval :BinaryClassificationEvaluator, cv_array: Array[(String, org.apache.spark.ml.tuning.CrossValidatorModel)], splits :Array[(org.apache.spark.sql.DataFrame, String)] ) :Array[evalRecord] = {\n   val eval_tuples = cv_array.map(cvmodel => {\n    val bestModel =  cvmodel._2.bestModel\n    val preds = splits.map(dataset => (bestModel.transform(dataset._1), cvmodel._2, dataset._2) )\n    val evalMetric = preds.map( preds => ( preds._1, preds._2, cvmodel._1, preds._3, binEval.evaluate(preds._1) ) ) //(predictions[DF], model[String], dataset[String], evalMetric[Double])\n                               evalMetric}\n                                  ).flatMap(x =>  x).sortBy(-_._5).map(x => evalRecord(x._1, x._2, x._3, x._4, x._5))  \n    eval_tuples\n}\n\nval cv_eval = CVEvaluatorBundle(binEval, cv_array, splits)\ncv_eval"
        }, 
        {
            "cell_type": "code", 
            "execution_count": 36, 
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "text": "Full List of models: \n+---------+-------+------------------+\n|modelDesc|dataset|        evalMetric|\n+---------+-------+------------------+\n|    combi|  train|0.9766666666666665|\n|    1gram|  train| 0.958122489959839|\n|      w2v|  train|0.9057329317269073|\n|      w2v|   test|0.8877911079745943|\n|    2gram|  train|0.8461144578313252|\n|    1gram|   test|0.8380969183721478|\n|    combi|   test|0.7928134556574926|\n|    2gram|   test|0.7754645965655139|\n+---------+-------+------------------+\n\nBest Performant model in Test Dataset: \n+---------+-------+------------------+\n|modelDesc|dataset|        evalMetric|\n+---------+-------+------------------+\n|      w2v|   test|0.8877911079745943|\n+---------+-------+------------------+\n\n", 
                    "name": "stdout"
                }
            ], 
            "source": "val evalReport = cv_eval.map(record =>  (record.modelDesc, record.dataset, record.evalMetric) )\nval evalReport_df = spark.createDataFrame(evalReport).toDF(\"modelDesc\", \"dataset\", \"evalMetric\")\nprintln(\"Full List of models: \")\nevalReport_df.show()\nprintln(\"Best Performant model in Test Dataset: \")\nevalReport_df.filter($\"dataset\" === lit(\"test\")).orderBy($\"evalMetric\".desc).limit(1).show()"
        }, 
        {
            "cell_type": "markdown", 
            "source": "Indeed, the best performant model in test dataset is not the one with best CV evaluation metrics (1gram, rf with maxDetph=6) but a word2vec (vector size of 20, rf with maxDetph=3)\n", 
            "metadata": {}
        }, 
        {
            "cell_type": "markdown", 
            "source": "### Word2Vec Exploratory Analysis", 
            "metadata": {}
        }, 
        {
            "cell_type": "code", 
            "execution_count": 45, 
            "metadata": {}, 
            "outputs": [], 
            "source": "val pl_model :PipelineModel = bestModel_w2v.asInstanceOf[PipelineModel]\nval w2v_model :Word2VecModel = pl_model.stages(0).asInstanceOf[Word2VecModel]"
        }, 
        {
            "cell_type": "code", 
            "execution_count": 47, 
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "text": "+------------+------------------+\n|        word|        similarity|\n+------------+------------------+\n|       state| 0.851428330806497|\n|       power|0.8225366866719859|\n|        said|0.7708845933927029|\n|    industry|0.7412072444116364|\n|   wholesale|0.7246787905599046|\n|  california|0.7244068972467216|\n| electricity|0.7077743453046441|\n|     markets|0.7022909007839018|\n|deregulation|0.6939859421820851|\n|    agreeing| 0.693162339732354|\n+------------+------------------+\n\n", 
                    "name": "stdout"
                }
            ], 
            "source": "println(\"Top ten most similar words to energy in w2v feature space: \")\nw2v_model.findSynonyms(\"energy\", 10).show()"
        }, 
        {
            "cell_type": "code", 
            "execution_count": 48, 
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "text": "Top ten most similar words to energy in w2v feature space: \n+------------+------------------+\n|        word|        similarity|\n+------------+------------------+\n| electricity|0.9284407403160926|\n|       state|0.9007867689385296|\n|deregulation|0.8960683589269083|\n|     markets|0.8602079498272223|\n|        said| 0.853785631969866|\n|   utilities|0.8537180649752815|\n|   supplying|0.8535447682240689|\n|     western|0.8519880326027605|\n|    industry|0.8516263994010073|\n| politicians|0.8496277609003039|\n+------------+------------------+\n\n", 
                    "name": "stdout"
                }
            ], 
            "source": "println(\"Top ten most similar words to power in w2v feature space: \")\nw2v_model.findSynonyms(\"power\", 10).show()"
        }, 
        {
            "cell_type": "code", 
            "execution_count": null, 
            "metadata": {}, 
            "outputs": [], 
            "source": ""
        }
    ], 
    "nbformat": 4, 
    "metadata": {
        "kernelspec": {
            "language": "scala", 
            "name": "scala-spark21", 
            "display_name": "Scala 2.11 with Spark 2.1"
        }, 
        "language_info": {
            "pygments_lexer": "scala", 
            "mimetype": "text/x-scala", 
            "name": "scala", 
            "version": "2.11.8", 
            "file_extension": ".scala", 
            "codemirror_mode": "text/x-scala"
        }
    }, 
    "nbformat_minor": 1
}